{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import scipy.linalg\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "#np.core.arrayprint._line_width = 80\n",
    "\n",
    "np.set_printoptions(edgeitems=30, linewidth=250, \n",
    "    formatter=dict(float=lambda x: \"% .4f\" % x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Useful fuctions \n",
    "def Euclid(x, y):\n",
    "    '''\n",
    "        d = Euclid(x, y) \n",
    "        Computes the Euclidean distance between the two vectors x and y \n",
    "        x and y are arrays of real numbers\n",
    "        d is the Euclidean distance between the two arrays\n",
    "    '''\n",
    "    ans = np.sqrt( np.sum( np.square(x-y) ) )\n",
    "    return(ans)\n",
    "\n",
    "def TensEuclid(x, y):\n",
    "    '''\n",
    "    d = TensEuclid(x, y)\n",
    "    Computes the average Euclidean distance between all vectors in a tensor \n",
    "    x and y are tensors of real numbers\n",
    "    d is the average Euclidean distance between the vectors in the tensor (like MSE but with sqrt)\n",
    "    used to get an idea of error, or how far off each vector is from its target\n",
    "    '''\n",
    "    \n",
    "    n = len(x)\n",
    "    if n != len(y):\n",
    "        print(\"input tensors must be of same length\")\n",
    "    else:\n",
    "        err = []\n",
    "        for i in range(n):\n",
    "            err.append(Euclid(x[i], y[i]))\n",
    "    return sum(err)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Useful functions\n",
    "def IsScalar(x):\n",
    "    if type(x) in (list, np.ndarray,):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def Thresh(x):\n",
    "    if IsScalar(x):\n",
    "        val = 1 if x>0 else -1\n",
    "    else:\n",
    "        val = np.ones_like(x)\n",
    "        val[x<0] = -1.\n",
    "    return val\n",
    "\n",
    "def ham(X, Y):\n",
    "    mul = np.multiply(X, Y)\n",
    "    H = 0\n",
    "    for i in mul:\n",
    "        if i<=0:\n",
    "            H=H+1\n",
    "            \n",
    "    return H\n",
    "    \n",
    "def Hamming(X, Y):\n",
    "    '''\n",
    "     H = Hamming(X, Y)\n",
    "     \n",
    "     Computes the Hamming distance between rows of X and Y.\n",
    "     If X and/or Y have multiple rows, then the output will\n",
    "     be an array.\n",
    "     \n",
    "     Inputs:\n",
    "      X   (M,D) array\n",
    "      Y   (N,D) array\n",
    "      \n",
    "     Output:\n",
    "      H   (M,N) array\n",
    "    '''\n",
    "    H = np.zeros((len(X), len(Y)), dtype=int)\n",
    "    for i,x in enumerate(X):\n",
    "        for j,y in enumerate(Y):\n",
    "            H[i,j] = np.sum(x*y<=0.)\n",
    "    return H\n",
    "\n",
    "def Perturb(x, p=0., n=0):\n",
    "    '''\n",
    "        y = Perturb(x, p=0., n=0)\n",
    "        \n",
    "        Apply binary noise to x. With probability p, each bit will be randomly\n",
    "        set to -1 or 1.\n",
    "        \n",
    "        Inputs:\n",
    "          x is an array of binary vectors of {-1,1}\n",
    "          p is the probability of each bit being randomly flipped (n=0)\n",
    "          n is the number of bits to flip (p=0)\n",
    "        \n",
    "        Output:\n",
    "          y is an array of binary vectors of {-1,1}\n",
    "    '''\n",
    "    y = copy.deepcopy(x)\n",
    "    for yy in y:\n",
    "        if p!=0.:\n",
    "            for k in range(len(yy)):\n",
    "                if np.random.rand()<p:\n",
    "                    yy[k] = Thresh(np.random.randint(2)*2-1)\n",
    "        if n>0:\n",
    "            k = np.random.randint(len(yy))\n",
    "            yy[k] *= -1\n",
    "    return y\n",
    "\n",
    "def lse(beta, x):\n",
    "    val = np.sum(np.exp(beta*x))\n",
    "    return np.log(val) / beta\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/( 1 + np.exp(-x) )\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def lif(x):\n",
    "    x_out = [0 if xi <= 1 else (1/( 0.5 - np.log( 1 - (1/xi)) )) for xi in x]\n",
    "    return x_out\n",
    " \n",
    "\n",
    "def V_train(X, N_batch):\n",
    "    ''' \n",
    "    X = self.Zeta ('correct dataset of patterns')\n",
    "    N_e = dimesion of encoded hidden layer \n",
    "    \n",
    "        returns a dataset to use for training decoding weights. \n",
    "        the dataset will be of size (N_samples, N_v) where N_v is the length of one pattern vector \n",
    "        it will include multiple copies of X as well as perturbed versions of it\n",
    "        (usually N_samples is \"a magnitide larger than N_e\" \n",
    "    '''\n",
    "    Nh, Nv = np.shape(X)    \n",
    "    V_t = X\n",
    "    for i in range( int( 0.80*(N_batch-1) ) ):\n",
    "        V_t = np.vstack((V_t, X))\n",
    "    for i in range(int( 0.20*(N_batch) ) ):\n",
    "        V_t = np.vstack( (V_t, X + np.random.normal(0, 0.2, size=X.shape)) )          \n",
    "    #print('samples:', np.shape(V_t)[0])\n",
    "    return V_t    \n",
    "\n",
    "\n",
    "#Computes the magnitudes of the rows in a matrix and returns the maximum and average magnitudes\n",
    "def max_mag(x):\n",
    "    samples, bits = x.shape\n",
    "    mags = []\n",
    "    for i in range(samples):\n",
    "        mag = np.sqrt(np.sum(np.square(x[i])))\n",
    "        mags.append(mag)\n",
    "    #print('Magnitudes:', mags)\n",
    "    max_mag = np.max(mags)\n",
    "    avg_mag = np.average(mags)\n",
    "    return max_mag, avg_mag\n",
    "\n",
    "\n",
    "def plot_state(t, v, I):\n",
    "    Nv = v.shape[1]\n",
    "    t0, tf = t[0], t[-1]\n",
    "    II = np.vstack([I,I]).T\n",
    "    plt.plot([t0, tf], II.T, ':')\n",
    "    plt.plot(t, v);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original LSE Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Simple version\n",
    "class LSEHopfield(object):\n",
    "    '''\n",
    "     net = ExpHopfield(Zeta)\n",
    "     \n",
    "     Creates a Hopfield network that uses LSE energy.\n",
    "     Zeta holds the target patterns in rows.\n",
    "    '''\n",
    "    def __init__(self, Zeta, beta=1., tau=1., kappa=1., decaytype=\"Off\", split_time=1.5):\n",
    "        self.Zeta = copy.deepcopy(Zeta)\n",
    "        self.Nh, self.Nv = self.Zeta.shape\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "        self.tau = tau\n",
    "        self.decaytype = decaytype\n",
    "        self.split_time = split_time\n",
    "        \n",
    "    def dynamics(self, I):\n",
    "        '''\n",
    "         f = net.dynamics(I)\n",
    "\n",
    "         Returns a dynamics function of the form\n",
    "           f(t, z)\n",
    "         that can be used in the IVP solvers.\n",
    "         \n",
    "         I is the input to the network.\n",
    "        '''\n",
    "        \n",
    "        def func(t, z):\n",
    "            '''\n",
    "             z = [v, h]\n",
    "             v, h, f are all N-D. b and c are scalar\n",
    "            '''\n",
    "            v = z[               :self.Nv        ]\n",
    "            h = z[self.Nv        :self.Nv+self.Nh]\n",
    "            \n",
    "            # Changing I to different targets \n",
    "            self.I = I\n",
    "            if len(self.I) == 2:\n",
    "                if t < self.split_time:\n",
    "                    self.I = self.I[0]\n",
    "                if t >= self.split_time:\n",
    "                    self.I = self.I[1]\n",
    "            dh = ( self.Zeta@v - h ) / self.tau\n",
    "            \n",
    "            #No decay of I \n",
    "            if self.decaytype == \"Off\":\n",
    "                if t <= self.split_time: \n",
    "                    Beta = 0.5 \n",
    "                if t > self.split_time:\n",
    "                    Beta = 0\n",
    "                dv = ( (1-Beta)*self.Zeta.T@softmax(h) - self.kappa*v + Beta*self.I ) / self.tau \n",
    "            \n",
    "            #Decay of I\n",
    "            if self.decaytype == \"Linear\":\n",
    "                dv = ( self.Zeta.T@softmax(h) - self.kappa*v + self.I*(1-self.beta*t) ) / self.tau  \n",
    "            if self.decaytype == \"Expo\":\n",
    "                Beta = np.exp(-self.beta*t)\n",
    "                dv =  ( (1-Beta)*self.Zeta.T@softmax(h) - self.kappa*v + self.I*(Beta) ) / self.tau\n",
    " \n",
    "            dzdt = np.concatenate((dv, dh))\n",
    "            return dzdt\n",
    "        return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIO LSE Network (no learning, no encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BioLSEHopfield(object):\n",
    "    '''\n",
    "     net = ExpHopfield(Zeta, tau=1, tau_sm=1, kappa=1)\n",
    "     \n",
    "     Creates a Hopfield network that uses LSE energy and biologically plausible implementation of Softmax.\n",
    "      \n",
    "      Zeta holds the target patterns in rows.\n",
    "      tau = time constant for the v and h nodes.\n",
    "      tau_sm = time constant for the f and c nodes (for computing softmax)\n",
    "      (tau_sm < tau)\n",
    "      kappa is the decay coefficient for v\n",
    "    '''\n",
    "    def __init__(self, Zeta, beta=1., tau_v=1., tau_h=1., tau_sm=1., kappa=1., decaytype=\"Off\", T=1):\n",
    "        self.Zeta = copy.deepcopy(Zeta)\n",
    "        self.Nh, self.Nv = self.Zeta.shape\n",
    "        self.tau_v = tau_v\n",
    "        self.tau_h = tau_h\n",
    "        self.tau_sm = tau_sm\n",
    "        self.kappa = kappa\n",
    "        self.beta = beta\n",
    "        self.decaytype = decaytype\n",
    "        self.T = T\n",
    "        \n",
    "    def dynamics(self, I):\n",
    "        '''\n",
    "         f = net.dynamics(I)\n",
    "\n",
    "         Returns a dynamics function of the form\n",
    "           f(t, z)\n",
    "         that can be used in the IVP solvers.\n",
    "         \n",
    "         I is the input to the network.\n",
    "        '''\n",
    "        \n",
    "        def func(t, z):\n",
    "            '''\n",
    "             z = [v, h, f, b]\n",
    "             v, h, f are all N-D. b is a scalar\n",
    "            '''\n",
    "            v = z[               :self.Nv        ]\n",
    "            h = z[self.Nv        :self.Nv+self.Nh]\n",
    "            f = z[self.Nv+self.Nh:-1             ]\n",
    "            c = z[-1]\n",
    "\n",
    "            T = self.T\n",
    "            \n",
    "            \n",
    "            #No decay of I \n",
    "            if self.decaytype == \"Off\":\n",
    "                if t < T:\n",
    "                    Beta = 0.5\n",
    "                if t>= T:\n",
    "                    Beta = 0\n",
    "                dv = ( (1-Beta)*self.Zeta.T@np.exp(f) - self.kappa*v + Beta*I ) / self.tau_v\n",
    "                #dv = ( self.Zeta.T@softmax(h) - v + I ) / self.tau\n",
    "            \n",
    "            #Exponential decay of \n",
    "            if self.decaytype == \"Expo\":\n",
    "                Beta = np.exp(-self.beta*t)\n",
    "                dv = ( (1-Beta)*self.Zeta.T@np.exp(f) - self.kappa*v + I*(Beta) ) / self.tau_v\n",
    "            \n",
    "            \n",
    "            dh = ( self.Zeta@v - h ) / self.tau_h\n",
    "            df = ( h - c - f ) / self.tau_sm\n",
    "            dc = ( np.log(np.sum(np.exp(h))) - c ) / self.tau_sm\n",
    "\n",
    "            dzdt = np.concatenate((dv, dh, df, [dc]))\n",
    "            \n",
    "            return dzdt\n",
    "        \n",
    "        return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIO LSE Network (with learning, no encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BioLSEHopfield_Learn(object):\n",
    "    '''\n",
    "     net = BioLSEHopfield_Learn(Zeta, beta, tau_w=1, tau_h=1, tau_sm=1, tau_I=1, kappa=1, decaytype, batchsize)\n",
    "     \n",
    "      Creates a Hopfield network that uses LSE energy and biologically plausible implementation of Softmax.\n",
    "      \n",
    "      Option to use network to learn weights (training=True) or use network normally (training=False)\n",
    "      \n",
    "      Zeta holds the target patterns in rows.\n",
    "      beta is the decay constant for the input (to have input decay set decaytype='Expo')\n",
    "      tau_w = time constant for weights or for the v nodes when training is off)\n",
    "      tau_h = time constant for h nodes\n",
    "      tau_sm = time constant for the f and c nodes (for computing softmax) (tau_sm < tau_h <tau_w)\n",
    "      kappa is the decay coefficient for v\n",
    "      decay_type (of input) can be \"Off\" means no decay, decay_type = \"Expo\" means exponential decay \n",
    "      \n",
    "      If you want to use the network for sample, set batchsize=1. Batchsize defaults to the entire dataset.\n",
    "    '''\n",
    "    def __init__(self, Zeta, beta=1., tau_w=1., tau_h=1., tau_sm=1., t_I=1., kappa=1., decaytype=\"Off\", batchsize=None):\n",
    "        self.Zeta = copy.deepcopy(Zeta)\n",
    "        self.Nh, self.Nv = self.Zeta.shape\n",
    "        if batchsize is None:\n",
    "            self.bs = self.Nh\n",
    "        else:\n",
    "            self.bs = 1\n",
    "        self.tau_w = tau_w\n",
    "        self.tau_h = tau_h\n",
    "        self.tau_sm = tau_sm\n",
    "        self.t_I = t_I\n",
    "        self.kappa = kappa\n",
    "        self.beta = beta\n",
    "        self.decaytype = decaytype\n",
    "        self.I = 0.    # This is just here to address the presence of I with Beta\n",
    "    \n",
    "    def pack(self, v, h, f, c, X):\n",
    "        '''\n",
    "         z = net.pack(v, h, f, c, X)\n",
    "         Can be undone using v,h,f,c,X = net.unpack(z).\n",
    "        '''\n",
    "        if self.bs==1:\n",
    "            \n",
    "            X=X.ravel().reshape((self.Nh*self.Nv, 1))\n",
    "            return np.concatenate((v,h,f,c.T,X), axis=0).ravel()\n",
    "        else:\n",
    "            return np.concatenate((v,h,f,c.T,X), axis=0).ravel()\n",
    "    \n",
    "    def unpack(self, z):\n",
    "        '''\n",
    "         v,h,f,c,X = net.unpack(z)\n",
    "         Can be undone using net.pack(v, h, f, c, X).\n",
    "        '''\n",
    "        Nh = self.Nh\n",
    "        Nv = self.Nv\n",
    "        bs = self.bs\n",
    "        v = z[ : Nv*bs]\n",
    "        hstart = len(v)\n",
    "        h = z[hstart : hstart + Nh*bs]\n",
    "        fstart = hstart + len(h)\n",
    "        f = z[fstart : fstart + Nh*bs]\n",
    "        cstart = fstart + len(f)\n",
    "        c = z[cstart : cstart + bs]\n",
    "        Xstart = cstart + len(c)\n",
    "        X = z[Xstart :]\n",
    "\n",
    "        v = v.reshape( (Nv, bs) )\n",
    "        h = h.reshape( (Nh, bs) )\n",
    "        f = f.reshape( (Nh, bs) )\n",
    "        X = X.reshape( (Nv, Nh) )\n",
    "        c = c.reshape( (bs, 1 ) )\n",
    "        \n",
    "        return v, h, f, c, X\n",
    "    \n",
    "    def dynamics(self, t, z, training):\n",
    "        '''\n",
    "         z = [v, h, f, c, X]\n",
    "\n",
    "         During training weights (training = True):\n",
    "         dv and dh are set to zero (so v and h do not change)\n",
    "         v and h should be set to the targets and 1-hot respectively by setting their components\n",
    "         in the ititial z array (y_0) to these values and setting their derivative to zero\n",
    "         f, c, X, all follow the dynamical equations below.\n",
    "         @equilibrium, f = h - LSE(h) (Nh x Nh),  c = LSE(h) (Nh x 1), X = Zeta.T (Nh x Nv)\n",
    "\n",
    "\n",
    "         During testing (training = False):\n",
    "         dX is set to zero (so X will not change)\n",
    "         X is fixed to be the weights by setting initial z array (y_0) to these values and setting dXdt to zero.\n",
    "         v, h, f, c all follow their dynamical equations.\n",
    "         @equilibrium, f = h - LSE(h) (Nh x Nh),  c = LSE(h) (Nh x 1), v = (1/kappa)*(Zeta.T softmax(h) + I) \n",
    "        '''\n",
    "        v,h,f,c,X = self.unpack(z)        \n",
    "\n",
    "        Nh = self.Nh\n",
    "        Nv = self.Nv\n",
    "        bs = self.bs\n",
    "        \n",
    "        if training:\n",
    "            #if we want to learn the weights, clamp v and h by setting initial values to targets and 1-hot and dvdt=dhdt=0. \n",
    "            dv = np.zeros_like(v)\n",
    "            dh = np.zeros_like(h)\n",
    "            df = ( h - c.T - f) / self.tau_sm\n",
    "            dc = ( np.log(np.sum(np.exp(h), axis=0, keepdims=True)).T - c ) / self.tau_sm\n",
    "            dX = ( v @ np.exp(f) - X )/ self.tau_w\n",
    "        \n",
    "        else:\n",
    "            #if we want to actually use the network, we clamp X to the weights by setting this in the initial values and dXdt=0.\n",
    "            #this means we will take 1 target at a time, so the dimensions reduce here\n",
    "            \n",
    "            dX = np.zeros_like(X)\n",
    "           \n",
    "        \n",
    "            #### Decay Equations ####\n",
    "            #No decay\n",
    "            #if self.decaytype == \"Off\":\n",
    "                #dv = ( X@np.exp(f) - self.kappa*v + self.I ) / self.tau_w\n",
    "            #Exponential decay\n",
    "            #if self.decaytype == \"Expo\":\n",
    "                #Beta = np.exp(-self.beta*t)\n",
    "                #dv = ( (1-Beta)*X@np.exp(f) - self.kappa*v + self.I*(Beta) ) / self.tau_w\n",
    "                \n",
    "            if t<self.t_I: \n",
    "                I = self.I\n",
    "                Beta=0.5\n",
    "            if t>=self.t_I:\n",
    "                I = 0\n",
    "                Beta = 0\n",
    "    \n",
    "            dv = ( (1-Beta)*X@np.exp(f) - self.kappa*v + I*(Beta) ) / self.tau_w                \n",
    "            dh = ( X.T@v - h ) / self.tau_h\n",
    "            df = ( h - c.T - f) / self.tau_sm\n",
    "            dc = (( np.log(np.sum(np.exp(h), axis=0))).reshape((bs,1)) - c ) / self.tau_sm\n",
    "            \n",
    "        dzdt = self.pack(dv, dh, df, dc, dX)\n",
    "        \n",
    "        return dzdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIO LSE Network (no learning, with encoded hidden representation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BioLSEHopfield_Encode(object):\n",
    "    '''\n",
    "     net = BioLSEHopfield_Encode()\n",
    "     \n",
    "      Creates a Hopfield network that uses LSE energy and biologically plausible implementation of Softmax \n",
    "      and a higher dimensional hidden representation.\n",
    "      \n",
    "      Nh_e = Dimensions of encoded hidden layer\n",
    "      enc_w = 'binary' or 'real' sets the encoding weights to be either binary values or real values respectively. \n",
    "      N_samp = Number of target samples in training dataset for decoding weights\n",
    "    '''\n",
    "    def __init__(self, Zeta, Nh_e, beta=1., tau_w=1., tau_h=1., tau_sm=1., t_I =1., kappa=1., decaytype=\"Off\", enc_w = 'binary', N_batch=1, batchsize=None):\n",
    "        \n",
    "        self.Zeta = copy.deepcopy(Zeta)       \n",
    "        self.Nh, self.Nv = self.Zeta.shape\n",
    "        self.Nh_e = Nh_e \n",
    "        self.N_batch = N_batch\n",
    "        if batchsize is None:\n",
    "            self.bs = self.Nh\n",
    "        else:\n",
    "            self.bs = 1\n",
    "        self.tau_w = tau_w\n",
    "        self.tau_h = tau_h\n",
    "        self.tau_sm = tau_sm\n",
    "        self.t_I = t_I\n",
    "        self.kappa = kappa\n",
    "        self.beta = beta\n",
    "        self.decaytype = decaytype\n",
    "        self.I = 1    \n",
    "        self.D_h = 1.\n",
    "        self.D_LSEh=1.\n",
    "        self.enc_w = enc_w\n",
    "        self.drop = np.random.choice(self.Nh_e, int(0.75*self.Nh_e), replace=False) #for dropping a certain amount of neurons in the encoded hidden layer\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### ENCODING & LEARNING DECODERS###\n",
    "        \n",
    "        #Create samples to train decoders with \n",
    "        V = V_train(self.Zeta, self.N_batch)\n",
    "        \n",
    "        #Encoding weights & bias (binary)\n",
    "        h_rad = max_mag(XX@XX.T)[0]\n",
    "        if self.enc_w == 'binary':\n",
    "            self.W_e = h_rad*(2*(np.random.binomial(n=1, p=0.5, size=(self.Nh, self.Nh_e))) - 1) \n",
    "            self.B_e = 2*(np.random.binomial(n=1, p=0.5, size=(self.Nh_e,))) - 1 \n",
    "        #Encoding weights & bias (real)\n",
    "        if self.enc_w == 'real':\n",
    "            self.W_e = self.Nv*np.sqrt(self.Nh)*np.random.normal(0, 1, size=(self.Nh, self.Nh_e))               \n",
    "            self.B_e = np.random.normal(0, 1, size=(self.Nh_e,))      \n",
    "\n",
    "        #Activities of hidden layer\n",
    "        Ah = V@self.Zeta.T\n",
    "        \n",
    "        LSE_Ah = np.log( np.sum( np.exp(Ah), axis=1 ) )\n",
    "\n",
    "        \n",
    "        #Project/encode the hidden layer\n",
    "        Acth = sigmoid(Ah@self.W_e + self.B_e)\n",
    "\n",
    "        #Want to decode A and LSE(A) from Act (learn the decoders)\n",
    "        self.D_h = np.linalg.lstsq(Acth, Ah, rcond=None)[0].T\n",
    "        self.D_LSEh = np.linalg.lstsq(Acth, LSE_Ah, rcond=None)[0].T\n",
    "\n",
    "            \n",
    "    def pack(self, v, h, f, c, X):\n",
    "        '''\n",
    "         z = net.pack(v, h, f, c, X)\n",
    "         Can be undone using v,h,f,c,X = net.unpack(z).\n",
    "        '''\n",
    "        if self.bs==1:\n",
    "            \n",
    "            X=X.ravel().reshape((self.Nh*self.Nv, 1))\n",
    "            return np.concatenate((v,h,f,c.T,X), axis=0).ravel()\n",
    "        else:\n",
    "            return np.concatenate((v,h,f,c.T,X), axis=0).ravel()\n",
    "    \n",
    "    def unpack(self, z):\n",
    "        '''\n",
    "         v,h,f,c,X = net.unpack(z)\n",
    "         Can be undone using net.pack(v, h, f, c, X).\n",
    "        '''\n",
    "        Nh = self.Nh\n",
    "        Nv = self.Nv\n",
    "        bs = self.bs\n",
    "        v = z[ : Nv*bs]\n",
    "        hstart = len(v)\n",
    "        h = z[hstart : hstart + Nh*bs]\n",
    "        fstart = hstart + len(h)\n",
    "        f = z[fstart : fstart + Nh*bs]\n",
    "        cstart = fstart + len(f)\n",
    "        c = z[cstart : cstart + bs]\n",
    "        Xstart = cstart + len(c)\n",
    "        X = z[Xstart :]\n",
    "\n",
    "        v = v.reshape( (Nv, bs) )\n",
    "        h = h.reshape( (Nh, bs) )\n",
    "        f = f.reshape( (Nh, bs) )\n",
    "        X = X.reshape( (Nv, Nh) )\n",
    "        c = c.reshape( (bs, 1 ) )\n",
    "        \n",
    "        return v, h, f, c, X\n",
    "    \n",
    "    def dynamics(self, I):\n",
    "        '''\n",
    "         f = net.dynamics(I)\n",
    "\n",
    "         Returns a dynamics function of the form\n",
    "           f(t, z)\n",
    "         that can be used in the IVP solvers.\n",
    "         \n",
    "         I is the input to the network.\n",
    "        '''\n",
    "        self.I = I\n",
    "        \n",
    "        def func(t, z):\n",
    "            '''\n",
    "             z = [v, h_e, f, c]\n",
    "             v, h, f are all N-D. b and c are scalar\n",
    "            '''\n",
    "\n",
    "            v =   z[                 : self.Nv        ]\n",
    "            h_e = z[self.Nv          : self.Nv+self.Nh_e]\n",
    "            f =   z[self.Nv+self.Nh_e: self.Nv+self.Nh_e+self.Nh]\n",
    "            c =   z[-1]\n",
    "            \n",
    "            \n",
    "            dh_e = ( sigmoid( self.W_e.T@(self.Zeta@v) + self.B_e ) - h_e ) / self.tau_h\n",
    "            \n",
    "            #uncomment below to turn off of the neurons in the projected hidden layer (randomly selected)\n",
    "            #dh_e=[0 if ind in self.drop else x for ind, x in enumerate(dh_e)]\n",
    "            \n",
    "            if t<self.t_I: \n",
    "                self.I = self.I\n",
    "                Beta=1\n",
    "            if t>=self.t_I:\n",
    "                self.I = 0\n",
    "                Beta = 0\n",
    "                \n",
    "            dv = ( (1-Beta)*self.Zeta.T@np.exp(f) - self.kappa*v + self.I*(Beta) ) / self.tau_w    \n",
    "            \n",
    "            dc = ( self.D_LSEh@h_e - c )/self.tau_sm\n",
    "            \n",
    "            df = ( self.D_h@h_e - c - f ) / self.tau_sm\n",
    "             \n",
    "            dzdt = np.concatenate((dv, dh_e, df, [dc]))\n",
    "            return dzdt\n",
    "        \n",
    "        return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIO LSE HN with encoded hidden representation and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioLSE_Encode_Learn(object):\n",
    "    '''\n",
    "     net = BioLSE_Encode_Learn()\n",
    "     \n",
    "      Creates a Hopfield network that uses LSE energy and biologically plausible implementation of Softmax \n",
    "      and a higher dimensional hidden representation.\n",
    "      \n",
    "      Nh_e = Dimensions of encoded hidden layer\n",
    "      enc_w = 'binary' or 'real' sets the encoding weights to be either binary values or real values respectively. \n",
    "      N_samp = Number of target samples in training dataset for decoding weights\n",
    "      \n",
    "      Option to use network to learn weights (training=True) or use network normally (training=False)\n",
    "      \n",
    "    '''\n",
    "    def __init__(self, Zeta, Nh_e, beta=1., tau_w=1., tau_h=1., tau_sm=1., t_I =1., kappa=1., decaytype=\"Off\", enc_w = 'binary', N_batch=1, batchsize=None):\n",
    "        \n",
    "        self.Zeta = Zeta      \n",
    "        self.Nh, self.Nv = self.Zeta.shape\n",
    "        self.Nh_e = Nh_e \n",
    "        self.N_batch = N_batch\n",
    "        if batchsize is None:\n",
    "            self.bs = self.Nh\n",
    "        else:\n",
    "            self.bs = 1\n",
    "        self.tau_w = tau_w\n",
    "        self.tau_h = tau_h\n",
    "        self.tau_sm = tau_sm\n",
    "        self.t_I = t_I\n",
    "        self.kappa = kappa\n",
    "        self.beta = beta\n",
    "        self.decaytype = decaytype\n",
    "        self.I = 1    \n",
    "        self.D_h = 1.\n",
    "        self.D_LSEh=1.\n",
    "        self.enc_w = enc_w\n",
    "        #self.drop = np.random.choice(self.Nh_e, int(0.75*self.Nh_e), replace=False) #for dropping a certain amount of neurons in the encoded hidden layer\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### ENCODING & LEARNING DECODERS###\n",
    "        \n",
    "        #Create samples to train decoders with \n",
    "        V = V_train(self.Zeta, self.N_batch).T\n",
    "        #print(\"V_tr shape:\", V.shape)\n",
    "        #Encoding weights & bias (binary)\n",
    "        h_rad = max_mag(XX@XX.T)[0]\n",
    "        if self.enc_w == 'binary':\n",
    "            self.W_e = h_rad*(2*(np.random.binomial(n=1, p=0.5, size=(self.Nh_e, self.Nh))) - 1) \n",
    "            #self.B_e = 2*(np.random.binomial(n=1, p=0.5, size=(self.Nh*self.N_batch, self.Nh_e))) - 1 \n",
    "        #Encoding weights & bias (real)\n",
    "#         if self.enc_w == 'real':\n",
    "#             self.W_e = self.Nv*np.sqrt(self.Nh)*np.random.normal(0, 1, size=(self.Nh, self.Nh_e))               \n",
    "#             self.B_e = np.random.normal(0, 1, size=(self.Nh_e,))      \n",
    "\n",
    "        #Activities of hidden layer\n",
    "        #Ah = self.Zeta@V.T\n",
    "        #print(\"\\Xi shape:\", self.Zeta.shape)\n",
    "        H = self.Zeta @ V\n",
    "        #print(\"act of H:\", H.T[:10])\n",
    "        LSE_h = np.log( np.sum( np.exp(H), axis=0 ) )\n",
    "        #print('lse(h):', LSE_h[:10])\n",
    "        \n",
    "        #Project/encode the hidden layer\n",
    "        Ah = sigmoid(self.W_e@H)\n",
    "        #print(\"shape of act of encoded hidden:\", Ah.shape)\n",
    "        #Want to decode A and LSE(A) from Act (learn the decoders)\n",
    "        self.D_h = np.linalg.lstsq(Ah.T, H.T, rcond=None)[0].T\n",
    "        self.D_LSEh = np.linalg.lstsq(Ah.T, LSE_h.T, rcond=None)[0].T\n",
    "#         print(self.D_LSEh.shape)\n",
    "#         print('Decoded act of h:', (self.D_h@Ah).T[:10])\n",
    "#         print('Decoded LSE(h):', self.D_LSEh@Ah)\n",
    "        \n",
    "    def pack(self, v, h, f, c, X):\n",
    "        '''\n",
    "         z = net.pack(v, h, f, c, X)\n",
    "         Can be undone using v,h,f,c,X = net.unpack(z).\n",
    "        '''\n",
    "        if self.bs==1:\n",
    "            X=X.ravel().reshape((self.Nh*self.Nv, 1))\n",
    "            return np.concatenate((v,h,f,c.T,X), axis=0).ravel()\n",
    "        else:\n",
    "            c.reshape(1, self.Nh)\n",
    "            #print('shapes:', v.shape, h.shape, f.shape, c.shape, X.shape)\n",
    "            return np.concatenate((v,h,f,c,X), axis=0).ravel()\n",
    "    \n",
    "    def unpack(self, z):\n",
    "        '''\n",
    "         v,he,f,c,X = net.unpack(z)\n",
    "         Can be undone using net.pack(v, he, f, c, X).\n",
    "        '''\n",
    "\n",
    "        Nh = self.Nh\n",
    "        Nv = self.Nv\n",
    "        Nh_e = self.Nh_e\n",
    "        bs = self.bs\n",
    "        v = z[ : Nv*bs]\n",
    "        hstart = len(v)\n",
    "        he = z[hstart : hstart + Nh_e*bs]\n",
    "        fstart = hstart + len(he)\n",
    "        f = z[fstart : fstart + Nh*bs]\n",
    "        cstart = fstart + len(f)\n",
    "        c = z[cstart : cstart + bs]\n",
    "        Xstart = cstart + len(c)\n",
    "        X = z[Xstart :]\n",
    "\n",
    "        v = v.reshape( (Nv, bs) )\n",
    "        he = he.reshape( (Nh_e, bs) )\n",
    "        f = f.reshape( (Nh, bs) )\n",
    "        X = X.reshape( (Nv, Nh) )\n",
    "        c = c.reshape( (bs, 1 ) )\n",
    "        \n",
    "\n",
    "        \n",
    "        return v, he, f, c, X\n",
    "    \n",
    "    def dynamics(self, t, z, training):\n",
    "        '''\n",
    "         z = [v, h, f, c, X]\n",
    "\n",
    "         During training weights (training = True):\n",
    "         dv and dh are set to zero (so v and h do not change)\n",
    "         v and h should be set to the targets and 1-hot respectively by setting their components\n",
    "         in the ititial z array (y_0) to these values and setting their derivative to zero\n",
    "         f, c, X, all follow the dynamical equations below.\n",
    "         @equilibrium, f = h - LSE(h) (Nh x Nh),  c = LSE(h) (Nh x 1), X = Zeta.T (Nh x Nv)\n",
    "\n",
    "\n",
    "         During testing (training = False):\n",
    "         dX is set to zero (so X will not change)\n",
    "         X is fixed to be the weights by setting initial z array (y_0) to these values and setting dXdt to zero.\n",
    "         v, h, f, c all follow their dynamical equations.\n",
    "         @equilibrium, f = h - LSE(h) (Nh x Nh),  c = LSE(h) (Nh x 1), v = (1/kappa)*(Zeta.T softmax(h) + I) \n",
    "        '''\n",
    "        Nh = self.Nh\n",
    "        Nv = self.Nv\n",
    "        bs = self.bs\n",
    "    \n",
    "            \n",
    "    \n",
    "        if training: #training\n",
    "        #if we want to learn the weights, clamp v and h by setting initial values to targets and 1-hot and dvdt=dhdt=0. \n",
    "\n",
    "            v, h_e, f, c, X = self.unpack(z) \n",
    "#             print('v.shape:', v.shape)\n",
    "#             print('he.shape:', h_e.shape)\n",
    "#             print('f.shape:', f.shape)\n",
    "#             print('c.shape:', c, c.shape)\n",
    "#             print('X.shape:', X.shape)\n",
    "\n",
    "            #print(h)\n",
    "            #h_e = sigmoid(self.W_e@h)\n",
    "#             print('h_e.shape:', h_e.shape)\n",
    "#             print('decoders shape', self.D_h.shape, self.D_LSEh.shape)\n",
    "#             print('decoded h:', (self.D_h@h_e).T)\n",
    "        \n",
    "            dv = np.zeros_like(v)\n",
    "            dh_e = np.zeros_like(h_e)\n",
    "            df = ( self.D_h@h_e - c.T - f) / self.tau_sm\n",
    "            dc = ( self.D_LSEh@h_e - c.T ) / self.tau_sm\n",
    "            dX = ( v @ np.exp(f) - X )/ self.tau_w\n",
    "            #dX = (np.outer(np.exp(f), v)[0].reshape((Nv, Nh)) - X)/ self.tau_w \n",
    "        \n",
    "        \n",
    "        \n",
    "        else: #testing (not training) \n",
    "        #if we want to actually use the network, we clamp X to the weights by setting this in the initial values and dXdt=0.\n",
    "         #this means we will take 1 target at a time, so the dimensions reduce here\n",
    "            \n",
    "            v,h_e,f,c,X = self.unpack(z) \n",
    "#             v =   z[                 : self.Nv        ]\n",
    "#             h_e = z[self.Nv          : self.Nv+self.Nh_e]\n",
    "#             f =   z[self.Nv+self.Nh_e: self.Nv+self.Nh_e+self.Nh]\n",
    "#             c =   z[-1]\n",
    "            \n",
    "            #print('W_e.shape:', self.W_e.shape)\n",
    "            \n",
    "            \n",
    "            dX = np.zeros_like(X)\n",
    "            dh_e = ( sigmoid( self.W_e@(X.T@v) ) - h_e ) / self.tau_h\n",
    "            \n",
    "            #uncomment below to turn off of the neurons in the projected hidden layer (randomly selected)\n",
    "            #dh_e=[0 if ind in self.drop else x for ind, x in enumerate(dh_e)]\n",
    "            \n",
    "            if t<self.t_I: \n",
    "                self.I = self.I\n",
    "                Beta=1\n",
    "            if t>=self.t_I:\n",
    "                self.I = 0\n",
    "                Beta = 0\n",
    "                \n",
    "            dv = ( (1-Beta)*(X@np.exp(f)) - self.kappa*v + self.I*(Beta) ) / self.tau_w  \n",
    "\n",
    "            dc = ( self.D_LSEh@h_e - c )/self.tau_sm\n",
    "            \n",
    "            df = ( self.D_h@h_e - c - f ) / self.tau_sm\n",
    "\n",
    "        \n",
    "        dzdt = self.pack(dv, dh_e, df, dc, dX)\n",
    "        return dzdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest_Target Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Nearest_Targ_Exp(Nh, Nv, runs):\n",
    "    '''\n",
    "    Runs LSEHopfield [runs] times for a random binary input. \n",
    "    Goal is to show that the network converges to the pattern with the smallest hamming distance to the input.\n",
    "    Returns the rate at which this occurs\n",
    "    '''\n",
    "\n",
    "    closest = 0\n",
    "    closest_bio = 0\n",
    "    avg_hamming = 0\n",
    "    avg_hamming_change = 0 \n",
    "    \n",
    "    for i in range(runs):\n",
    "        print('Run:', i)\n",
    "        \n",
    "        ### Create data ###\n",
    "        X = np.random.binomial(n=1, p=0.5, size=(Nh, Nv))\n",
    "        XX = 2.*X - 1\n",
    "\n",
    "        \n",
    "        \n",
    "        ### Learn weights ###\n",
    "        tspan=[0, 3]\n",
    "        #Initialize network\n",
    "        net_bio_learn_all=BioLSEHopfield_Learn(XX, beta=1., tau_w=0.01, tau_h=0.01, tau_sm=0.0001, t_I=0, kappa=1., decaytype=\"Off\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Initial state for net\n",
    "        y0_bio_learn_all = (np.random.random(( (2*Nv) + (2*Nh) + 1 , Nh )) /50.).ravel()\n",
    "        #Clamp v and h to ideal values\n",
    "        v = XX.T.ravel()\n",
    "        h = np.eye(Nh).ravel()\n",
    "        h=h*20\n",
    "        y0_bio_learn_all[     : Nv*Nh        ] = v\n",
    "        y0_bio_learn_all[Nv*Nh: Nv*Nh + Nh*Nh] = h\n",
    "        \n",
    "        #Solve network to learn (of side of) the weights\n",
    "        sol_blearn_all = solve_ivp(net_bio_learn_all.dynamics, tspan, y0_bio_learn_all, method='RK45', args=(True,))\n",
    "        vf, hf, ff, cf, Xff = net_bio_learn_all.unpack(sol_blearn_all.y.T[-1])\n",
    "        #X_learned = weight_update(0.5, XX, 10000)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### RUN ###\n",
    "        tspan = [0, 2]\n",
    "        #Initialize network\n",
    "        #BIO LSE Network\n",
    "        net_bio = BioLSEHopfield_Learn(Xff.T, beta=10., tau_w=0.05, tau_h = 0.05, tau_sm=0.0001, t_I = 1, kappa=1, decaytype=\"Off\", batchsize=1)\n",
    "        #ORIGINAL LSE Network \n",
    "        net = LSEHopfield(XX, beta=1., tau=0.05, kappa=1., decaytype=\"Off\", split_time=1)\n",
    "        \n",
    "        #Initial state for net:\n",
    "        #For BIO LSE (using learned weights)\n",
    "        y0_bio_1 = np.random.random((Nv+2*Nh+Nh*Nv+1,)) / 10.\n",
    "        y0_bio_1[-Nh*Nv:] = Xff.ravel()\n",
    "        #For OG LSE\n",
    "        y0 = np.random.random((Nv+Nh,))/10\n",
    "        \n",
    "        #Set I to be a random binary input \n",
    "        I= np.random.binomial(n=1, p=0.5, size=(Nv,))\n",
    "        I = 2.*I - 1\n",
    "        #OR select a pattern and (optionally) perturb it\n",
    "#         k = np.random.randint(len(XX))\n",
    "#         II = XX[k] #Correct pattern\n",
    "#         I = Perturb( XX , n=0, p=0.40 ) #Perturbed the data (binary perturbations)\n",
    "#         I = I[k]\n",
    "        net_bio.I=I.reshape( (Nv, 1) )\n",
    "        \n",
    "        #How close is this input to all other targets? the network should converge to one of the closest targets\n",
    "        hams = Hamming([I.T], XX)\n",
    "        #print('hamming dist between I and patterns:', hams)\n",
    "        closest_patterns = np.flatnonzero(hams == hams.min())\n",
    "        \n",
    "        \n",
    "        #Solve network \n",
    "        sol_bio = solve_ivp(net_bio.dynamics, tspan, y0_bio_1, args=(False,))\n",
    "        sol = solve_ivp(net.dynamics(I), tspan, y0)\n",
    "        \n",
    "        #Store states of network \n",
    "        v_bio = sol_bio.y[:Nv,:].T\n",
    "        vf = sol.y[:Nv].T\n",
    "        hf = sol.y[Nv:].T\n",
    "        ##final states\n",
    "        vf_bio, hf_bio, ff_bio, cf_bio, Xf_bio = net_bio.unpack(sol_bio.y.T[-1])\n",
    "        \n",
    "        k_conv_bio = np.argmax(softmax(hf_bio))\n",
    "        k_conv = np.argmax(softmax(hf[-1]))\n",
    "        #print('hamming dist between this pattern and the input:', ham(vf, I.reshape((Nv,1))))\n",
    "        \n",
    "        if i%25 == 0:\n",
    "            print('closest patterns to I:', closest_patterns)\n",
    "            print('with a hamming distance of:', np.min(hams))\n",
    "            print('BIO LSE network converges to pattern', k_conv_bio)\n",
    "            print('OG LSE network converges to pattern', k_conv)\n",
    "\n",
    "            \n",
    "            #BIO LSE PLOT\n",
    "            plot_state(sol_bio.t, v_bio, I)\n",
    "            plt.axvline(x=1, c='grey', linestyle='--')\n",
    "            plt.axvspan(0, 1, color='0.9')\n",
    "            plt.title(f'Bio LSE Hopfield Network State Plot', color='black', size=14);\n",
    "            plt.xlabel('time (s)', color='black', size=12)\n",
    "            plt.tick_params(axis='x', colors='black')\n",
    "            plt.tick_params(axis='y', colors='black')\n",
    "            plt.ylabel('feature neuron (v) states ',color='black', size=12)\n",
    "            #plt.savefig('stateplt.pdf')\n",
    "            plt.show()\n",
    "            #OG LSE PLOT\n",
    "            plot_state(sol.t, vf, I)\n",
    "            plt.axvline(x=1, c='grey', linestyle='--')\n",
    "            plt.axvspan(0, 1, color='0.9')\n",
    "            plt.title(f'LSE Hopfield Network State Plot', color='black', size=14);\n",
    "            plt.xlabel('time (s)', color='black', size=12)\n",
    "            plt.tick_params(axis='x', colors='black')\n",
    "            plt.tick_params(axis='y', colors='black')\n",
    "            plt.ylabel('feature neuron (v) states ',color='black', size=12)\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "        if k_conv_bio in closest_patterns:\n",
    "            closest_bio += 1\n",
    "            avg_hamming += np.min(hams)\n",
    "            avg_hamming_change += ( np.min(hams) - ham(vf[-1], XX[k_conv]) ) \n",
    "        if k_conv in closest_patterns:\n",
    "             closest += 1\n",
    "        \n",
    "        #print()\n",
    "        #print()\n",
    "           \n",
    "    return closest_bio/runs, closest/runs, avg_hamming_change/runs, avg_hamming/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Nearest_Targ_ENCODED(Nh, Nv, N_E, Nsample, runs):\n",
    "    '''\n",
    "    Runs LSEHopfield [runs] times for a random binary input. \n",
    "    Goal is to show that the network converges to the pattern with the smallest hamming distance to the input.\n",
    "    Returns the rate at which this occurs\n",
    "    '''\n",
    "\n",
    "    closest = 0\n",
    "    avg_hamming = 0\n",
    "    avg_hamming_change = 0 \n",
    "    for i in range(runs):\n",
    "        print('Run:', i)\n",
    "        \n",
    "        #Create data\n",
    "        X = np.random.binomial(n=1, p=0.5, size=(Nh, Nv))\n",
    "        XX = 2.*X - 1\n",
    "          \n",
    "        ### RUN ###\n",
    "        \n",
    "        ### Initialize network\n",
    "        net_e = BioLSEHopfield_Encode(XX, N_E, beta=1., tau_w=0.1, tau_h=0.05, tau_sm=0.0001, t_I=1.0, kappa=1., \n",
    "                                      decaytype=\"Off\", enc_w = 'real', N_batch=Nsample)\n",
    "        #net_LSE = LSEHopfield(XX, beta=1., tau=0.1, kappa=1., decaytype=\"Off\", split_time=1.0)\n",
    "        \n",
    "        \n",
    "        ### Set I to be a random binary input \n",
    "        I = np.random.binomial(n=1, p=0.5, size=(Nv,))\n",
    "        I = 2.*I - 1\n",
    "        net_e.I=I\n",
    "        #How close is this input to all other targets? the network should converge to one of the closest targets\n",
    "        hams = Hamming([I.T], XX)\n",
    "        closest_patterns = np.flatnonzero(hams == hams.min())\n",
    "        print('closest patterns to I:', closest_patterns, \"with hamming of\", np.min(hams))\n",
    "        \n",
    "\n",
    "        ### Solve \n",
    "        tspan = [0, 3]\n",
    "        y0=np.random.random((net_e.Nh_e+net_e.Nv+net_e.Nh+1,) )/100\n",
    "        #y0 = np.random.random( (Nv+Nh,) )/100\n",
    "        sol = solve_ivp(net_e.dynamics(I), tspan, y0)\n",
    "        #sol_LSE = solve_ivp(net_LSE.dynamics(I), tspan, y0)\n",
    "        \n",
    "        ### Final states\n",
    "        v_e = sol.y[        :net_e.Nv].T\n",
    "        h_e = sol.y[net_e.Nv:net_e.Nv+net_e.Nh_e].T[-1]  \n",
    "        #v = sol_LSE.y[        :Nv].T\n",
    "        #h = sol_LSE.y[Nv:].T[-1]\n",
    "        k_conv = np.argmax(softmax(net_e.D_h@h_e))\n",
    "        #k_conv = np.argmax(softmax(h))\n",
    "        print('Net identifies pattern number as:', k_conv)\n",
    "        print('feature neurons stabilize to:', v_e[-1])\n",
    "        print('correct target pattern:', XX[closest_patterns])\n",
    "        #print(\"SM of encoded hidden representation:\", softmax(h_e), '\\n')\n",
    "        #print(\"SM of decoded 1hot representation:\", softmax(net_e.W_d@h_e))\n",
    "        print('Hamming distance between feature neurons and target pattern:', ham(v_e[-1], XX[k_conv]))\n",
    "        #print('Hamming distance between feature neurons and pattern',k_conv,\":\",ham(v[-1], XX[k_conv]))\n",
    "\n",
    "        \n",
    "      \n",
    "        \n",
    "        ### Uncomment below and set runs=1 to get an example plot\n",
    "        plot_state(sol.t, v_e, I)\n",
    "        plt.axvline(x=1.0, c='grey', linestyle='--')\n",
    "        plt.axvspan(0, 1.0, color='0.9')\n",
    "        plt.title(f'Bio LSE HN (Encoded) State Plot', color='black', size=14);\n",
    "        plt.xlabel('time (s)', color='black', size=12)\n",
    "        plt.tick_params(axis='x', colors='black')\n",
    "        plt.tick_params(axis='y', colors='black')\n",
    "        plt.ylabel('feature neuron (v) states ',color='black', size=12)\n",
    "        plt.savefig('stateplt.pdf')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        if k_conv in closest_patterns:\n",
    "            closest += 1\n",
    "            avg_hamming += np.min(hams)\n",
    "            avg_hamming_change += ( np.min(hams) - ham(v_e[-1], XX[k_conv]) ) \n",
    "            \n",
    "           \n",
    "    return closest/runs, avg_hamming_change/runs, avg_hamming/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Nearest_Targ_Encode_Learn(Nh, Nhe, Nv, tI, runs):\n",
    "    '''\n",
    "    Runs BioLSE_Encode_Learn [runs] times for a random binary input:\n",
    "        In each run, a \"memory dataset\" of Nh targets with Nv bits each is constructed. This nework encodes the hidden representation in Nhe neurons.\n",
    "        The network learns the weights from the SM subnetwork to the feature neurons (f to v connection, Xi.T). \n",
    "        Once these weights are learned we provide a random target as input for tI seconds.\n",
    "        Once the input is turned OFF we check whether or not the network can fully recover the closest target from the memory dataset \n",
    "    \n",
    "    Goal is to show that the network converges to the target pattern with the smallest hamming distance to the input.\n",
    "    Returns the rate at which this occurs\n",
    "    \n",
    "    '''\n",
    "\n",
    "    closest = 0\n",
    "    closest_BE = 0\n",
    "    avg_hamming = 0\n",
    "    avg_hamming_change = 0 \n",
    "    \n",
    "    for i in range(runs):\n",
    "        print('Run:', i)\n",
    "        \n",
    "        ### Create data ###\n",
    "        X = np.random.binomial(n=1, p=0.5, size=(Nh, Nv))\n",
    "        XX = 2.*X - 1\n",
    "        \n",
    "        \n",
    "        ### Learn weights ###\n",
    "        tspan=[0, 0.5]\n",
    "        #Initialize network\n",
    "        net_BE_learn=BioLSE_Encode_Learn(XX, Nh_e = Nhe, beta=1., tau_w=0.1, tau_h=0.01, tau_sm=0.001, t_I=1, kappa=1., decaytype=\"Off\", enc_w = 'binary', N_batch = 500)\n",
    "\n",
    "\n",
    "        #Initial state for net\n",
    "        y0_BE_learn = (np.random.random( (2*Nv + Nhe + Nh + 1, Nh) ) / 100).ravel()\n",
    "        #Clamp v and h to ideal values\n",
    "        v = XX.T.ravel()\n",
    "        h = XX@XX.T\n",
    "        h_E =  (sigmoid(net_BE_learn.W_e@h)).ravel()\n",
    "        y0_BE_learn[     : Nv*Nh        ] = v\n",
    "        y0_BE_learn[Nv*Nh: Nv*Nh + Nh*Nhe] = h_E\n",
    "\n",
    "\n",
    "        #Solve network to learn the weights\n",
    "        sol_BE_learn = solve_ivp(net_BE_learn.dynamics, tspan, y0_BE_learn, args=(True,))\n",
    "        vf, hf, ff, cf, Xff = net_BE_learn.unpack(sol_BE_learn.y.T[-1])\n",
    "        print(Xff.T)\n",
    "        print(XX)\n",
    "        \n",
    "        \n",
    "        ### RUN ###\n",
    "        tspan = [0, 2]\n",
    "        #Initialize network\n",
    "        \n",
    "        #BIO LSE Network\n",
    "        net_BE = BioLSE_Encode_Learn(Xff.T, Nh_e = Nhe, beta=1., tau_w=0.05, tau_h = 0.05, tau_sm=0.0001, t_I = 1, kappa=1, decaytype=\"Off\", enc_w = 'binary', N_batch = 1000, batchsize=1)\n",
    "        #ORIGINAL LSE Network \n",
    "        #net = LSEHopfield(XX, beta=1., tau=0.05, kappa=1., decaytype=\"Off\", split_time=1)\n",
    "        \n",
    "        #Initial state for net:\n",
    "        #For BIO LSE (using learned weights)\n",
    "        y0_BE_test = np.random.random( (Nv + Nhe + Nh + Nh*Nv + 1,) ) / 10.\n",
    "        y0_BE_test[-Nh*Nv:] = Xff.ravel()\n",
    "        #For OG LSE\n",
    "        #y0 = np.random.random((Nv+Nh,))/10\n",
    "        \n",
    "        #Set I to be a random binary input \n",
    "        I= np.random.binomial(n=1, p=0.5, size=(Nv,))\n",
    "        I = 2.*I - 1\n",
    "        #OR select a pattern and (optionally) perturb it\n",
    "#         k = np.random.randint(len(XX))\n",
    "#         II = XX[k] #Correct pattern\n",
    "#         I = Perturb( XX , n=0, p=0.40 ) #Perturbed the data (binary perturbations)\n",
    "#         I = I[k]\n",
    "        net_BE.I=I.reshape( (Nv, 1) )\n",
    "        \n",
    "        #How close is this input to all other targets? the network should converge to one of the closest targets\n",
    "        hams = Hamming([I.T], XX)\n",
    "        #print('hamming dist between I and patterns:', hams)\n",
    "        closest_patterns = np.flatnonzero(hams == hams.min())\n",
    "        \n",
    "        \n",
    "        #Solve network \n",
    "        sol_BE = solve_ivp(net_BE.dynamics, tspan, y0_BE_test, args=(False,))\n",
    "        #sol = solve_ivp(net.dynamics(I), tspan, y0)\n",
    "        \n",
    "#         #Store states of network \n",
    "#         v_bio = sol_bio.y[:Nv,:].T\n",
    "#         vf = sol.y[:Nv].T\n",
    "#         hf = sol.y[Nv:].T\n",
    "        ##final states\n",
    "        vf_BE, hf_BE, ff_BE, cf_BE, Xf_BE = net_BE.unpack(sol_BE.y.T[-1])\n",
    "        print(vf_BE)\n",
    "        print(hf_BE)\n",
    "        \n",
    "        \n",
    "        \n",
    "        k_conv_BE = np.argmax(softmax(net_BE.D_h@hf_BE))\n",
    "        #k_conv = np.argmax(softmax(hf[-1]))\n",
    "        #print('hamming dist between this pattern and the input:', ham(vf, I.reshape((Nv,1))))\n",
    "        \n",
    "        if i%25 == 0:\n",
    "            print('closest patterns to I:', closest_patterns)\n",
    "            print('with a hamming distance of:', np.min(hams))\n",
    "            print('BIO LSE network converges to pattern', k_conv_BE)\n",
    "            #print('OG LSE network converges to pattern', k_conv)\n",
    "\n",
    "            \n",
    "            #BIO LSE PLOT\n",
    "            plot_state(sol_BE.t, vf_BE, I)\n",
    "            plt.axvline(x=1, c='grey', linestyle='--')\n",
    "            plt.axvspan(0, 1, color='0.9')\n",
    "            plt.title(f'Bio LSE HN (Encoded Hidden Rep.) State Plot', color='black', size=14);\n",
    "            plt.xlabel('time (s)', color='black', size=12)\n",
    "            plt.tick_params(axis='x', colors='black')\n",
    "            plt.tick_params(axis='y', colors='black')\n",
    "            plt.ylabel('feature neuron (v) states ',color='black', size=12)\n",
    "            #plt.savefig('stateplt.pdf')\n",
    "            plt.show()\n",
    "#             #OG LSE PLOT\n",
    "#             plot_state(sol.t, vf, I)\n",
    "#             plt.axvline(x=1, c='grey', linestyle='--')\n",
    "#             plt.axvspan(0, 1, color='0.9')\n",
    "#             plt.title(f'LSE Hopfield Network State Plot', color='black', size=14);\n",
    "#             plt.xlabel('time (s)', color='black', size=12)\n",
    "#             plt.tick_params(axis='x', colors='black')\n",
    "#             plt.tick_params(axis='y', colors='black')\n",
    "#             plt.ylabel('feature neuron (v) states ',color='black', size=12)\n",
    "#             plt.show()\n",
    "\n",
    "    \n",
    "        if k_conv_BE in closest_patterns:\n",
    "            closest_BE += 1\n",
    "            avg_hamming += np.min(hams)\n",
    "            avg_hamming_change += ( np.min(hams) - ham(vf[-1], XX[k_conv]) ) \n",
    "#         if k_conv in closest_patterns:\n",
    "#              closest += 1\n",
    "        \n",
    "        #print()\n",
    "        #print()\n",
    "           \n",
    "    return closest_BE/runs, avg_hamming_change/runs, avg_hamming/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING\n",
    "Can we learn the weights using the BIO LSE network with the encoded hidden rep.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9943 -0.8052 -0.6101  0.9978]\n",
      " [ 0.8912  0.9265  0.9590  0.9989]\n",
      " [-0.9829 -0.9660  0.8577  0.9996]\n",
      " [-0.9829 -0.9660  0.8577  0.9996]\n",
      " [-0.9709  0.7161 -0.9094  1.0014]]\n",
      "[[-1.0000 -1.0000 -1.0000  1.0000]\n",
      " [ 1.0000  1.0000  1.0000  1.0000]\n",
      " [-1.0000 -1.0000  1.0000  1.0000]\n",
      " [-1.0000 -1.0000  1.0000  1.0000]\n",
      " [-1.0000  1.0000 -1.0000  1.0000]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.binomial(n=1, p=0.5, size=(5, 4))\n",
    "XX = 2.*X - 1\n",
    "\n",
    "Nh, Nv = XX.shape\n",
    "Nh_E = 500\n",
    "### Learn weights ###\n",
    "tspan=[0, 1]\n",
    "#Initialize network\n",
    "net_BE_learn=BioLSE_Encode_Learn(XX, Nh_e = Nh_E, beta=1., tau_w=0.01, tau_h=0.0005, tau_sm=0.0001, \n",
    "                                 t_I=1, kappa=1., decaytype=\"Off\", enc_w = 'binary', N_batch = 50)\n",
    "\n",
    "\n",
    "#Initial state for net\n",
    "y0_BE_learn = (np.random.random( (2*Nv + Nh_E + Nh + 1, Nh) ) / 100).ravel()\n",
    "#Clamp v and h to ideal values\n",
    "v = XX.T.ravel()\n",
    "h = XX@XX.T\n",
    "h_E =  (sigmoid(net_BE_learn.W_e@h)).ravel()\n",
    "y0_BE_learn[     : Nv*Nh        ] = v\n",
    "y0_BE_learn[Nv*Nh: Nv*Nh + Nh*Nh_E] = h_E\n",
    "\n",
    "#Solve network to learn the weights\n",
    "sol_BE_learn = solve_ivp(net_BE_learn.dynamics, tspan, y0_BE_learn, args=(True,))\n",
    "vf, hf, ff, cf, Xff = net_BE_learn.unpack(sol_BE_learn.y.T[-1])\n",
    "print(Xff.T)\n",
    "print(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0000  1.0000 -1.0000 -1.0000 -1.0000]\n",
      " [-1.0000  1.0000 -1.0000 -1.0000  1.0000]\n",
      " [-1.0000  1.0000  1.0000  1.0000 -1.0000]\n",
      " [ 1.0000  1.0000  1.0000  1.0000  1.0000]] (4, 5)\n",
      "[[ 200.0000  0.0000  0.0000  0.0000  0.0000]\n",
      " [ 0.0000  200.0000  0.0000  0.0000  0.0000]\n",
      " [ 0.0000  0.0000  200.0000  0.0000  0.0000]\n",
      " [ 0.0000  0.0000  0.0000  200.0000  0.0000]\n",
      " [ 0.0000  0.0000  0.0000  0.0000  200.0000]]\n"
     ]
    }
   ],
   "source": [
    "v = XX.T\n",
    "h = (XX@XX.T)\n",
    "h2 = np.eye(Nh)\n",
    "h2 = h2*200\n",
    "print(v, v.shape)\n",
    "print(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0 ()\n",
      "[[ 200.0000  0.0000  0.0000  0.0000  0.0000]\n",
      " [ 0.0000  200.0000  0.0000  0.0000  0.0000]\n",
      " [ 0.0000  0.0000  200.0000  0.0000  0.0000]\n",
      " [ 0.0000  0.0000  0.0000  200.0000  0.0000]\n",
      " [ 0.0000  0.0000  0.0000  0.0000  200.0000]] [[ 0.0000 -200.0000 -200.0000 -200.0000 -200.0000]\n",
      " [-200.0000  0.0000 -200.0000 -200.0000 -200.0000]\n",
      " [-200.0000 -200.0000  0.0000 -200.0000 -200.0000]\n",
      " [-200.0000 -200.0000 -200.0000  0.0000 -200.0000]\n",
      " [-200.0000 -200.0000 -200.0000 -200.0000  0.0000]]\n",
      "[[ 1.0000  0.0000  0.0000  0.0000  0.0000]\n",
      " [ 0.0000  1.0000  0.0000  0.0000  0.0000]\n",
      " [ 0.0000  0.0000  1.0000  0.0000  0.0000]\n",
      " [ 0.0000  0.0000  0.0000  1.0000  0.0000]\n",
      " [ 0.0000  0.0000  0.0000  0.0000  1.0000]]\n",
      "[[-1.0000  1.0000 -1.0000 -1.0000 -1.0000]\n",
      " [-1.0000  1.0000 -1.0000 -1.0000  1.0000]\n",
      " [-1.0000  1.0000  1.0000  1.0000 -1.0000]\n",
      " [ 1.0000  1.0000  1.0000  1.0000  1.0000]]\n",
      "[[-1.0000  1.0000 -1.0000 -1.0000 -1.0000]\n",
      " [-1.0000  1.0000 -1.0000 -1.0000  1.0000]\n",
      " [-1.0000  1.0000  1.0000  1.0000 -1.0000]\n",
      " [ 1.0000  1.0000  1.0000  1.0000  1.0000]]\n"
     ]
    }
   ],
   "source": [
    "c = np.log(np.sum(np.exp(h2[0]), axis = 0)) \n",
    "print(c, c.shape)\n",
    "f = (h2 - c)\n",
    "print(h2, f)\n",
    "print(np.exp(f))\n",
    "X_l = v @ np.exp(f)\n",
    "X = XX.T\n",
    "print(X_l)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
